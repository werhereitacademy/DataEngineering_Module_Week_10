# DataEngineering_Module_Week_10

üöÄ Real-Time Weather Data Streaming: Kafka (Python + Docker)This project implements a complete real-time data streaming pipeline. It involves a Python Producer fetching data from the WeatherAPI, publishing it to a local Kafka cluster managed by Docker, and then consuming the stream using both a basic Python consumer and advanced Spark Structured Streaming (via Microsoft Fabric).‚öôÔ∏è Prerequisites and SetupBefore starting the project, ensure you have the following tools installed and configured:1. RequirementsToolVersion/LinkInstallationDocker DesktopLatesthttps://www.docker.com/products/docker-desktopPython3.9 or higher(Local installation required)Python Librarieskafka-python, requestsRun: pip install kafka-python requests2. Setting Up the Docker EnvironmentFollow these steps to deploy the Kafka Broker and UI locally:Create a project folder: kafka_demo on your desktop.Place the provided docker-compose.yml file inside this folder.Open your terminal and navigate to the folder:Bashcd Desktop\kafka_demo
Start the services in detached mode (-d):Bashdocker compose up -d
ComponentAddressPurposeKafka Brokerlocalhost:9092Main connection point.Kafka UIhttp://localhost:8081Interface for configuration and monitoring.Public Broker Portlocalhost:9093Used for external access (Ngrok).To stop the containers:Bashdocker compose down
üåê External ConnectivityTo allow external services like Microsoft Fabric to access the local Kafka broker, we use Ngrok.3. WeatherAPI ConfigurationThe producer will fetch data using this endpoint:http://api.weatherapi.com/v1/current.json?key={API_KEY}&q=Amsterdam&aqi=no
Action: Get a free API key from https://www.weatherapi.com/ and replace {API_KEY} in your producer code.4. Public Access with Ngrok (for Fabric Spark)Setup: Create an Ngrok account (https://dashboard.ngrok.com) and install it on your machine.Start Tunnel: Run the following command to expose port 9093:Bashngrok tcp 9093
‚ö†Ô∏è Note: Ngrok requires credit card verification for TCP tunnels, but you will not be charged.Get Address: Note the Ngrok address provided (e.g., tcp://6.tcp.eu.ngrok.io:17090).Update Docker: Add this full Ngrok address to the KAFKA_ADVERTISED_LISTENERS section of your docker-compose.yml file and restart Kafka.üìù Project TasksThe project includes three main components to be developed.üß© Task 1: Data Producer (Python)File: Create a Python file (producer.py).Data Fields: Retrieve the following current weather fields from WeatherAPI:CityTemperatureHumidityWind speedLocal timeLast updatedAction: Send the JSON-formatted data to the Kafka topic every 60 seconds.üñ•Ô∏è Task 2: Consumer 1 (Python)File: Create a separate Python file (consumer_python.py).Action: Read the real-time data from the Kafka topic and print only the current wind speed to the console.‚ö° Task 3: Consumer 2 (Spark Structured Streaming - Fabric)Platform: Microsoft Fabric Portal (New Notebook).Process:Start a Spark session.Use Structured Streaming to read the data using the Ngrok address.Automatically add a timestamp to each incoming record.Calculate the average temperature over a 5-minute window.Set the stream to slide (move) the window every 1 minute (the average updates every minute).Save the results to your Lakehouse in Delta format under the table name avg_temperature.
