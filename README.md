# DataEngineering_Module_Week_10

# ğŸ“˜ Real-Time Data Streaming with Kafka (Python + Docker)

## 1. Requirements
- [Docker Desktop](https://www.docker.com/products/docker-desktop) must be installed  
- Python **3.9+** must be installed  
- Install required Python libraries:
  ```bash
  pip install kafka-python requests
2. Docker Environment Setup
Create a folder named kafka_demo on your desktop.

Copy the docker-compose.yml file (provided by your instructor) into this folder.

Open a terminal and navigate to the folder:

bash
Copy code
cd Desktop/kafka_demo
Start Kafka and its UI:

bash
Copy code
docker compose up -d
Services:

ğŸ§© Kafka Broker â†’ localhost:9092

ğŸŒ Kafka UI â†’ http://localhost:8081

ğŸ”— Kafka Public Broker â†’ localhost:9093

To stop services:

bash
Copy code
docker compose down
3. WeatherAPI Settings
API Endpoint:

pgsql
Copy code
http://api.weatherapi.com/v1/current.json?key={API_KEY}&q=Amsterdam&aqi=no
Replace {API_KEY} with your own API key.

Get a free API key from WeatherAPI.

4. Public Access with Ngrok
To make data readable by Fabric (public access):

Create an account at Ngrok Dashboard.

Install Ngrok on your computer.

Run this command:

bash
Copy code
ngrok tcp 9093
âš ï¸ Note:
To use the TCP tunnel, you must add your credit card info under
Settings â†’ Account in your Ngrok dashboard. This is only for identity verification â€” you wonâ€™t be charged.

Ngrok will provide an address like:

cpp
Copy code
tcp://<ngrok-host>:<ngrok-port>
Example: tcp://6.tcp.eu.ngrok.io:17090
Use this address in your Docker configuration under:

nginx
Copy code
KAFKA_ADVERTISED_LISTENERS
5. Tasks
ğŸ§© Producer
Create a Python file.

Fetch data from WeatherAPI including:

City

Temperature

Humidity

Wind speed

Local time

Last updated

Send this data to Kafka every 60 seconds.

ğŸ–¥ï¸ Consumer 1 â€“ Python
Create another Python file.

It should read real-time data from Kafka and display the current wind speed in the console.

âš¡ Consumer 2 â€“ Spark
In Fabric, create a new Notebook.

Start a Spark session.

Use Structured Streaming to read data from Ngrok.

Add an automatic timestamp to each record.

Calculate the average temperature within a 5-minute window.

Slide the window every 1 minute to update the average dynamically.

Save the results in Delta format to the Lakehouse table named:

nginx
Copy code
avg_temperature
ğŸ§  Summary
This project demonstrates:

Real-time data streaming using Kafka

Data ingestion from WeatherAPI

Public data access with Ngrok

Real-time analytics in Spark Structured Streaming

ğŸ§© Folder Structure Example
Copy code
kafka_demo/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ producer.py
â”œâ”€â”€ consumer_python.py
â”œâ”€â”€ consumer_spark_notebook.ipynb
â””â”€â”€ README.md
âœ¨ Author
Prepared as part of a Kafka Real-Time Streaming Demo (Python + Docker + Spark).
